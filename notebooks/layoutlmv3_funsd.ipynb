{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LayoutLM applied to FUNSD dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main reference: https://github,com/philschmid/document-ai-transformers/blob/main/training/layoutlm_funsd,ipynb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first step is to install the Hugging Face Libraries, including transformers and datasets, Running the following cell will install all the required packages, Additinoally, we need to install an OCR-library to extract text from images, We will use pytesseract,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ubuntu\n",
    "#%sudo apt install -y tesseract-ocr\n",
    "# python\n",
    "# !pip3 install pytesseract transformers datasetsseqeval tensorboard evaluate tesseract\n",
    "\n",
    "# install git-fls for pushing model and logs to the hugging face hub\n",
    "#%sudo apt-get install git-lfs --yes\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we login into <code>huggingface_hub</code> and declare our access token, This will be useful later on when we fine-tune our pre-trained model,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid.\n",
      "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
      "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
      "\n",
      "git config --global credential.helper store\n",
      "\n",
      "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
      "Token has not been saved to git credential helper.\n",
      "Your token has been saved to /home/ubuntu/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "from datasets import load_dataset, Dataset, Features, Value, ClassLabel, Sequence\n",
    "import torch\n",
    "\n",
    "access_token = \"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\"\n",
    "notebook_login()\n",
    "\n",
    "file_to_open = '/home/ubuntu/perso/document_ai-2/data/processed/DocLayNet/multimodal/test.json'\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the FUNSD dataset a collection of 199 fully annotated forms, The dataset is available on Hugging Face at <code>nielsr/funsd</code>,\n",
    "\n",
    "The LayoutLM model doesn't have a <code>AutoProcessor</code> to nice create the our input documents, but we can use the <code>LayoutLMv2Processor</code> instead,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSOR_ID= \"microsoft/layoutlmv2-base-uncased\"\n",
    "DATA_PATH = \"../data/processed/DocLayNet/multimodal/\"\n",
    "PNG_PATH = \"../data/raw/DocLayNet/PNG/\"\n",
    "\n",
    "PART = 'test'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare FUNSD dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the funsd dataset, we use the <code>load_dataset()</code> method from the 🤗 Datasets library,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['Caption', 'Footnote', 'Formula', 'List-Item', 'Page-Footer', 'Page-Header', 'Picture','Section-Header', 'Table', 'Text', 'Title']\n",
    "\n",
    "\n",
    "features = Features({'id': Value(dtype='int64', id=None),\n",
    "    'bboxes': Sequence(feature=Sequence(feature=Value(dtype='float64', id=None), length=-1, id=None), length=-1, id=None),\n",
    "    'words': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
    "    'tags': Sequence(ClassLabel(num_classes=11, names=classes, id=None), length=-1, id=None),\n",
    "    'image_path': Value(dtype='string', id=None)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/perso/document_ai-2/data/processed/DocLayNet/multimodal/test.json'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_to_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf2cda1153b64916a1621fa8a5abd1da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-20df1e53e826e2f0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/ubuntu/.cache/huggingface/datasets/json/default-20df1e53e826e2f0/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c57f38383e1402c921916fee464b513",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fd83a5cb8cd4d07a9624d248ae4fad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad3e09ce70644aa0a7e90065748545cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/ubuntu/.cache/huggingface/datasets/json/default-20df1e53e826e2f0/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2888c1d999424f1285520995286b35d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset size: 4900\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "import numpy as np \n",
    "import base64\n",
    "\n",
    "\n",
    "def load_data_from_json(file_to_open) :\n",
    "    # Open the JSON file\n",
    "    with open(file_to_open, 'r') as f:\n",
    "        # Read the contents of the file\n",
    "        file_contents = f.read()\n",
    "\n",
    "    # Split the file contents by newline\n",
    "    json_objects = file_contents.split('\\n')\n",
    "\n",
    "    # # Load each JSON object and store it in a list\n",
    "    data = []\n",
    "    for obj in json_objects:\n",
    "        if obj.strip() != '':\n",
    "            # change ' per \"\n",
    "            data.append(json.loads(obj))\n",
    "                \n",
    "    # data = data[:1000]\n",
    "    data_dict_from_data = {\n",
    "        'id': [d['id'] for d in data],\n",
    "        'bboxes': [d['bboxes'] for d in data],\n",
    "        'words': [d['words'] for d in data],\n",
    "        'tags': [d['tags'] for d in data],\n",
    "        'image_path': [PNG_PATH+d['image_path'] for d in data]}\n",
    "\n",
    "    # # select the first 20 images\n",
    "    data_dict_from_data['image_path'] = data_dict_from_data['image_path']\n",
    "    data_dict_from_data['id'] = data_dict_from_data['id']\n",
    "    data_dict_from_data['bboxes'] = data_dict_from_data['bboxes']\n",
    "    data_dict_from_data['words'] = data_dict_from_data['words']\n",
    "    data_dict_from_data['tags'] = data_dict_from_data['tags']\n",
    "\n",
    "    dataset = Dataset.from_dict(data_dict_from_data, features=features)\n",
    "    return dataset\n",
    "\n",
    "dataset = load_data_from_json(file_to_open)\n",
    "dataset.to_json('/home/ubuntu/perso/document_ai-2/data/processed/DocLayNet/multimodal/test_multi.json')\n",
    "\n",
    "# dataset =  load_data_from_json(file_to_open.replace('test', 'val'))\n",
    "# dataset.to_json('/home/ubuntu/perso/document_ai-2/data/processed/DocLayNet/multimodal/val_multi.json')\n",
    "\n",
    "\n",
    "dataset = load_dataset('json', data_files={PART: DATA_PATH+'test_multi.json'}, features=features)\n",
    "print(f\"Test dataset size: {len(dataset[PART])}\")\n",
    "dataset['test'].features\n",
    "print(len(dataset))\n",
    "\n",
    "# dataset_val = load_dataset('json', data_files={PART: DATA_PATH+'val_multi.json'}, features=features)\n",
    "# print(f\"Val dataset size: {len(dataset_val[PART])}\")\n",
    "# dataset_val['test'].features\n",
    "# print(len(dataset_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Leigh Taliaferro, M.D.', 'General Surgeon', 'Abilene, Texas', '', 'The Abilene native started his practice 17 years ago', 'and has developed a flourishing business as a', 'general surgeon. He estimates that 90 percent of', 'his practice is for abdominal surgery. With such a', 'busy practice, he finds comfort in having a reliable', 'banking partner. “I have almost every type of busi-', 'ness, trust and personal account with First National', 'Bank of Abilene,” says Dr. Taliaferro. ', '“First National is immersed in this city – everywhere', 'you go, they are involved with helping people with', 'their business. It’s because of the people who', 'work there – they are leaders … generous people', 'who make their mark on the bank and on the', 'community. While they may be the biggest bank', 'in town, they sure don’t act like it. It’s like bank-', '“First National is immersed in this city – everywhere', 'you go, they are involved with helping people with', 'their business. It’s because of the people who', 'work there – they are leaders … generous people', 'who make their mark on the bank and on the', 'community. While they may be the biggest bank', 'in town, they sure don’t act like it. It’s like bank-', 'ing with friends.”', 'Dr. Taliaferro has invested in First Financial', 'Bankshares for more than a decade. “My stock has', 'done nothing but go up in value. They are solid,', 'sound businesspeople. I sleep well at night know-', 'ing that my investments are in good hands.”', 'Dr. Taliaferro has invested in First Financial', 'Bankshares for more than a decade. “My stock has', 'done nothing but go up in value. They are solid,', 'sound businesspeople. I sleep well at night know-', 'ing that my investments are in good hands.”', '“While they may be the biggest ', 'bank in town, they sure ', 'don’t act like it. ', 'It’s', 'like banking ', 'with friends.”', '8']\n",
      "[[45.9058352941, 6.8064012121, 83.9279393464, 15.388539798], [45.9058352941, 6.8064012121, 83.9279393464, 15.388539798], [45.9058352941, 6.8064012121, 83.9279393464, 15.388539798], [14.6405228758, 24.4987046465, 78.2061192157, 27.4845256566], [14.6405228758, 29.1935438384, 85.7427273203, 57.8897228283], [14.6405228758, 29.1935438384, 85.7427273203, 57.8897228283], [14.6405228758, 29.1935438384, 85.7427273203, 57.8897228283], [14.6405228758, 29.1935438384, 85.7427273203, 57.8897228283], [14.6405228758, 29.1935438384, 85.7427273203, 57.8897228283], [14.6405228758, 29.1935438384, 85.7427273203, 57.8897228283], [14.6405228758, 29.1935438384, 85.7427273203, 57.8897228283], [14.6405228758, 29.1935438384, 85.7427273203, 57.8897228283], [14.6405228758, 29.1935438384, 85.7427273203, 57.8897228283], [14.6405228758, 29.1935438384, 85.7427273203, 57.8897228283], [14.6405228758, 29.1935438384, 85.7427273203, 57.8897228283], [14.6405228758, 29.1935438384, 85.7427273203, 57.8897228283], [14.6405228758, 29.1935438384, 85.7427273203, 57.8897228283], [14.6405228758, 29.1935438384, 85.7427273203, 57.8897228283], [14.6405228758, 29.1935438384, 85.7427273203, 57.8897228283], [14.6405228758, 59.6267410101, 85.771056732, 88.3211410101], [14.6405228758, 59.6267410101, 85.771056732, 88.3211410101], [14.6405228758, 59.6267410101, 85.771056732, 88.3211410101], [14.6405228758, 59.6267410101, 85.771056732, 88.3211410101], [14.6405228758, 59.6267410101, 85.771056732, 88.3211410101], [14.6405228758, 59.6267410101, 85.771056732, 88.3211410101], [14.6405228758, 59.6267410101, 85.771056732, 88.3211410101], [14.6405228758, 59.6267410101, 85.771056732, 88.3211410101], [14.6405228758, 59.6267410101, 85.771056732, 88.3211410101], [14.6405228758, 59.6267410101, 85.771056732, 88.3211410101], [14.6405228758, 59.6267410101, 85.771056732, 88.3211410101], [14.6405228758, 59.6267410101, 85.771056732, 88.3211410101], [14.6405228758, 59.6267410101, 85.771056732, 88.3211410101], [14.6405228758, 90.6690626263, 85.8171853595, 108.3335046465], [14.6405228758, 90.6690626263, 85.8171853595, 108.3335046465], [14.6405228758, 90.6690626263, 85.8171853595, 108.3335046465], [14.6405228758, 90.6690626263, 85.8171853595, 108.3335046465], [14.6405228758, 90.6690626263, 85.8171853595, 108.3335046465], [24.5581949804, 136.9690480808, 213.1796946405, 204.9285565657], [24.5581949804, 136.9690480808, 213.1796946405, 204.9285565657], [24.5581949804, 136.9690480808, 213.1796946405, 204.9285565657], [24.5581949804, 136.9690480808, 213.1796946405, 204.9285565657], [24.5581949804, 136.9690480808, 213.1796946405, 204.9285565657], [24.5581949804, 136.9690480808, 213.1796946405, 204.9285565657], [4.0055006536, 219.6818626263, 5.6335267974, 221.9444888323]]\n",
      "[0, 0, 0, 7, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 4]\n",
      "../data/raw/DocLayNet/PNG/132a855ee8b23533d8ae69af0049c038171a06ddfcac892c3c6d7e6b4091c642.png\n"
     ]
    }
   ],
   "source": [
    "example = dataset['test'][0]\n",
    "words, boxes, ner_tags,image = example[\"words\"], example[\"bboxes\"], example[\"tags\"], example[\"image_path\"]\n",
    "print(words)\n",
    "print(boxes)\n",
    "print(ner_tags)\n",
    "print(image)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can process our dataset we need to define the <code>features</code> or the processed inputs, which are later based into the model, Features are a special dictionary that defines the internal structure of a dataset, Compared to traditional NLP datasets we need to add the <code>bbox</code> feature, which is a 2D array of the bounding boxes for each token,"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune and evaluate LayoutLM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have processed our dataset, we can start training our model, Therefore we first need to load the <code>microsoft/layoutlm-base-uncased</code> model with the <code>LayoutLMForTokenClassification</code> class with the label mapping of our dataset,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor\n",
    "\n",
    "# we'll use the Auto API here - it will load LayoutLMv3Processor behind the scenes,\n",
    "# based on the checkpoint we provide from the hub\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/layoutlmv3-base\", apply_ocr=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Caption', 'Footnote', 'Formula', 'List-Item', 'Page-Footer', 'Page-Header', 'Picture', 'Section-Header', 'Table', 'Text', 'Title']\n",
      "{0: 'Caption', 1: 'Footnote', 2: 'Formula', 3: 'List-Item', 4: 'Page-Footer', 5: 'Page-Header', 6: 'Picture', 7: 'Section-Header', 8: 'Table', 9: 'Text', 10: 'Title'}\n"
     ]
    }
   ],
   "source": [
    "from datasets.features import ClassLabel\n",
    "\n",
    "features = dataset[\"test\"].features\n",
    "column_names = dataset[\"test\"].column_names\n",
    "image_column_name = \"image_path\"\n",
    "text_column_name = \"words\"\n",
    "boxes_column_name = \"bboxes\"\n",
    "label_column_name = \"tags\"\n",
    "\n",
    "# In the event the labels are not a `Sequence[ClassLabel]`, we will need to go through the dataset to get the\n",
    "# unique labels.\n",
    "def get_label_list(labels):\n",
    "    unique_labels = set()\n",
    "    for label in labels:\n",
    "        unique_labels = unique_labels | set(label)\n",
    "    label_list = list(unique_labels)\n",
    "    label_list.sort()\n",
    "    return label_list\n",
    "\n",
    "if isinstance(features[label_column_name].feature, ClassLabel):\n",
    "    label_list = features[label_column_name].feature.names\n",
    "    # No need to convert the labels since they are already ints.\n",
    "    id2label = {k: v for k,v in enumerate(label_list)}\n",
    "    label2id = {v: k for k,v in enumerate(label_list)}\n",
    "else:\n",
    "    label_list = get_label_list(dataset[\"train\"][label_column_name])\n",
    "    id2label = {k: v for k,v in enumerate(label_list)}\n",
    "    label2id = {v: k for k,v in enumerate(label_list)}\n",
    "num_labels = len(label_list)\n",
    "print(label_list)\n",
    "print(id2label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f28af6e5aa764995aec45ea822930090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1950 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82e0a787a0984dc48bd5d2247cc203ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from io import BytesIO\n",
    "def prepare_examples(examples):\n",
    "  images = examples[image_column_name]\n",
    "  images = [Image.open(d).resize((224,224)) for d in images]\n",
    "  words = examples[text_column_name]\n",
    "  boxes = examples[boxes_column_name]\n",
    "  word_labels = examples[label_column_name]\n",
    "\n",
    "  encoding = processor(images, words, boxes=boxes, word_labels=word_labels,\n",
    "                       truncation=True, padding=\"max_length\",\n",
    "                       max_length=512)\n",
    "\n",
    "  return encoding\n",
    "\n",
    "from datasets import Features, Sequence, ClassLabel, Value, Array2D, Array3D\n",
    "\n",
    "# we need to define custom features for `set_format` (used later on) to work properly\n",
    "features = Features({\n",
    "    'pixel_values': Array3D(dtype=\"float32\", shape=(3, 224, 224)),\n",
    "    'input_ids': Sequence(feature=Value(dtype='int64')),\n",
    "    'attention_mask': Sequence(Value(dtype='int64')),\n",
    "    'bbox': Array2D(dtype=\"int64\", shape=(512, 4)),\n",
    "    'labels': Sequence(feature=Value(dtype='int64')),\n",
    "})\n",
    "\n",
    "# select the first 1000 examples of the datasets.arrow\n",
    "\n",
    "first_elem = dataset[\"test\"].select(range(3900))\n",
    "second = dataset[\"test\"].select(range(3900, len(dataset[\"test\"])))\n",
    "\n",
    "\n",
    "train_dataset = first_elem.map(\n",
    "    prepare_examples,\n",
    "    batched=True,\n",
    "    batch_size=2,\n",
    "    remove_columns=column_names,\n",
    "    features=features,\n",
    ")\n",
    "eval_dataset = second.map(\n",
    "    prepare_examples,\n",
    "    batched=True,\n",
    "    batch_size=2,\n",
    "    remove_columns=column_names,\n",
    "    features=features,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LayoutLMv3ForTokenClassification were not initialized from the model checkpoint at microsoft/layoutlmv3-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import LayoutLMv3ForTokenClassification\n",
    "# device = torch.device('cpu')\n",
    "model = LayoutLMv3ForTokenClassification.from_pretrained(\"microsoft/layoutlmv3-base\", num_labels=num_labels, label2id=label2id, id2label=id2label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to evaluate our model during training, The <code>Trainer</code> supports evaluation during training by providing a <code>compute_metrics</code>, We are going to use <code>seqeval</code> and the evaluate library to evaluate the overall f1 score for all tokens,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# load seqeval metric\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "# labels of the model\n",
    "ner_labels = list(model.config.id2label.values())\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    for prediction, label in zip(predictions, labels):\n",
    "        for predicted_idx, label_idx in zip(prediction, label):\n",
    "            if label_idx == -100:\n",
    "                continue\n",
    "            all_predictions.append(ner_labels[predicted_idx])\n",
    "            all_labels.append(ner_labels[label_idx])\n",
    "    return metric.compute(predictions=[all_predictions], references=[all_labels])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to define the hyperparameters (<code>TrainingArguments</code>) we want to use for our training, We are leveraging the Hugging Face Hub integration of the <code>Trainer</code> to automatically push our checkpoints, logs and metrics during training into a repository,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3900\n",
      "1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/perso/document_ai-2/notebooks/layoutlmv3-base-ner is already a clone of https://huggingface.co/Jacques2207/layoutlmv3-base-ner. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfFolder\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# hugging face parameter\n",
    "repository_id = \"layoutlmv3-base-ner\"\n",
    "\n",
    "# Define training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=repository_id,\n",
    "    num_train_epochs=5,#15\n",
    "    per_device_train_batch_size=2, # 16\n",
    "    per_device_eval_batch_size=4, # 8\n",
    "    fp16=False,\n",
    "    learning_rate=3e-5,\n",
    "    # logging & evaluation strategies\n",
    "    logging_dir=f\"{repository_id}/logs\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"overall_f1\",\n",
    "    # push to hub parameters\n",
    "    report_to=\"tensorboard\",\n",
    "    push_to_hub=True,\n",
    "    #hub_private_repo=True,\n",
    "    hub_strategy=\"every_save\",\n",
    "    hub_model_id=repository_id,\n",
    "    hub_token=\"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\",\n",
    "    #hub_token=HfFolder.get_token(),\n",
    ")\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(eval_dataset))\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    #tokenizer=tokenizer,\n",
    "    \n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start our training by using the train method of the <code>Trainer</code>,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3900\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 9750\n",
      "  Number of trainable parameters = 125926027\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/transformers/modeling_utils.py:810: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='9750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   3/9750 00:00 < 34:44, 4.68 it/s, Epoch 0.00/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9750, training_loss=1.1939457381810898, metrics={'train_runtime': 2454.3555, 'train_samples_per_second': 7.945, 'train_steps_per_second': 3.973, 'total_flos': 5175867280896000.0, 'train_loss': 1.1939457381810898, 'epoch': 5.0})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start training\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/transformers/models/layoutlmv3/processing_layoutlmv3.py:193: FutureWarning: `feature_extractor` is deprecated and will be removed in v5. Use `image_processor` instead.\n",
      "  warnings.warn(\n",
      "Image processor saved in layoutlmv3-base-ner/preprocessor_config.json\n",
      "tokenizer config file saved in layoutlmv3-base-ner/tokenizer_config.json\n",
      "Special tokens file saved in layoutlmv3-base-ner/special_tokens_map.json\n",
      "Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Token Classification', 'type': 'token-classification'}}\n",
      "Saving model checkpoint to layoutlmv3-base-ner\n",
      "Configuration saved in layoutlmv3-base-ner/config.json\n",
      "Model weights saved in layoutlmv3-base-ner/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c326617809a14831a8ed4abd5d1ac728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file logs/events.out.tfevents.1679302770.138-2-233-57.311419.0: 100%|##########| 8.08k/8.08k [00:00<?, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/Jacques2207/layoutlmv3-base-ner\n",
      "   50847ec..d88d405  main -> main\n",
      "\n",
      "Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Token Classification', 'type': 'token-classification'}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/Jacques2207/layoutlmv3-base-ner/commit/d88d4055cf51000d84f2719408b7998a0e64a6d1'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change apply_ocr to True to use the ocr text for inference\n",
    "processor.feature_extractor.apply_ocr = False\n",
    "\n",
    "# Save processor and create model card\n",
    "processor.save_pretrained(repository_id)\n",
    "trainer.create_model_card()\n",
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
